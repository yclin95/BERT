# -*- coding: utf-8 -*-
"""Copy of BERT_peer_review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ul0_N6AkPNI-I2Exb-KchugvKgqQucPr

<h2 align=center>Movie Review Classification with BERT & TensorFlow</h2>

In this [project](https://udemy.com/), you will learn how to fine-tune a BERT model for fake news classification using TensorFlow and TF-Hub.
"""

import tensorflow as tf
print(tf.version.VERSION)

!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git

# install requirements to use tensorflow/models repository
!pip install -Uqr models/official/requirements.txt
# you may have to restart the runtime afterwards

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import sys
sys.path.append('models')
from official.nlp.data import classifier_data_lib
from official.nlp.bert import tokenization
from official.nlp import optimization

!pip install official

print("TF Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print("Hub version: ", hub.__version__)
print("GPU is", "available" if tf.config.experimental.list_physical_devices("GPU") else "NOT AVAILABLE")

!pip install -q scipy==1.4.1

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('out1.csv',low_memory=False)
df.shape

!pip install -U pandas-profiling

df.tail(20)

df.accepted.plot(kind='hist', title='accepted distribution')
df.shape

train_df, remaining = train_test_split(df, random_state=42, train_size=0.8, stratify=df.accepted.values)
valid_df, test_df = train_test_split(remaining, random_state=42, train_size=0.5, stratify=remaining.accepted.values)


train_df.shape, valid_df.shape, test_df.shape

import tensorflow as tf

with tf.device('/cpu:0'):
  train_data = tf.data.Dataset.from_tensor_slices((train_df['clean_comments'].values, train_df['accepted'].values))
  valid_data = tf.data.Dataset.from_tensor_slices((valid_df['clean_comments'].values, valid_df['accepted'].values))
  test_data =  tf.data.Dataset.from_tensor_slices((test_df['clean_comments'].values, test_df['accepted'].values)) 
  for text, label in train_data.take(5):
    print(text)
    print(label)

"""
Each line of the dataset is composed of the review text and its label
- Data preprocessing consists of transforming text to BERT input features:
input_word_ids, input_mask, segment_ids
- In the process, tokenizing the text is done with the provided BERT model tokenizer
"""

 # Label categories
 # 1 - Insincere Question
 # 0 - Sincere Question
label_list = [0,1]


 # maximum length of (token) input sequences
max_seq_length = 128
train_batch_size = 32



# Get BERT layer and tokenizer:
# More details here: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2
bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2', trainable=True)
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)

# This provides a function to convert row to input features and label

def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):
  example = classifier_data_lib.InputExample(guid=None,
                                             text_a = text.numpy(),
                                             text_b = None,
                                             label = label.numpy())
  feature = classifier_data_lib.convert_single_example(0, example, label_list, max_seq_length, tokenizer)
  return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)

def to_feature_map(text, label):
  input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature, inp=[text,label], Tout=[tf.int32, tf.int32, tf.int32, tf.int32])
  input_ids.set_shape([max_seq_length])
  input_mask.set_shape([max_seq_length])
  segment_ids.set_shape([max_seq_length])
  label_id.set_shape([])
  x = {'input_words_ids' : input_ids,
       'input_mask': input_mask,
       'input_type_id': segment_ids
       }  
  return (x, label_id)

with tf.device('/cpu:0'):
  # train
  train_data = (train_data.map(to_feature_map,
                               num_parallel_calls=tf.data.experimental.AUTOTUNE)
  .shuffle(1000)
  .batch(32, drop_remainder=True)        
  .prefetch(tf.data.experimental.AUTOTUNE))

  # valid
  valid_data = (valid_data.map(to_feature_map,
                               num_parallel_calls=tf.data.experimental.AUTOTUNE)
  .batch(32, drop_remainder=True)        
  .prefetch(tf.data.experimental.AUTOTUNE))

# train data spec
train_data.element_spec

# valid data spec
valid_data.element_spec

# Building the model
def create_model():
  input_words_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_words_ids")
  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_mask")
  input_type_id = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_type_id")
  pooled_output, sequence_output = bert_layer([input_words_ids, input_mask, input_type_id])
  drop = tf.keras.layers.Dropout(0.4)(pooled_output)
  output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(drop)
  model = tf.keras.Model(
           inputs = {
               'input_words_ids': input_words_ids,
               'input_mask': input_mask,
               'input_type_id': input_type_id          
            },
           outputs = output )
  return model

model = create_model()
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-6),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy()])
model.summary()

tf.keras.utils.plot_model(model=model, show_shapes=True, dpi=76)

# Train model
class_weight = {0: 6,
                1: 1,
                }
epochs = 20
history = model.fit(train_data,
                    validation_data=valid_data,
                    epochs=epochs, class_weight=class_weight,
                    verbose=1)

"""## Task 11: Evaluate the BERT Text Classification Model"""

import matplotlib.pyplot as plt

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])
  plt.show()

plot_graphs(history, 'loss')

plot_graphs(history, 'binary_accuracy')

X_test=test_df['clean_comments']
y_test=test_df['accepted']
test_data = tf.data.Dataset.from_tensor_slices((X_test, [1]*len(X_test)))
test_data = test_data.map(to_feature_map).batch(1)
preds = model.predict(test_data)
threshold = 0.5
df1 = pd.DataFrame([1 if pred >= threshold else 0 for pred in preds])

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, df1)
print(cm)
from sklearn.metrics import accuracy_score
p=accuracy_score(y_test, df1)
from sklearn.metrics import f1_score
f1=f1_score(y_test, df1)
from sklearn.metrics import recall_score
recall=recall_score(y_test, df1)
print(p)
print(f1)
print(recall)

#导入要用的库
import sklearn.metrics as metrics 
from sklearn.metrics import roc_curve 
from sklearn.metrics import roc_auc_score as AUC
import matplotlib.pyplot as plt

fpr, tpr, thresholds = metrics.roc_curve(y_test, df1)
roc_auc=metrics.auc(fpr,tpr)

from sklearn.metrics import roc_curve
from sklearn.metrics import auc



# Plot ROC curve
plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate or (1 - Specifity)')
plt.ylabel('True Positive Rate or (Sensitivity)')
plt.title('ROC curve of BERT')
plt.legend(loc="lower right")











